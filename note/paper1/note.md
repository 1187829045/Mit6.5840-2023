MapReduce 是一种编程模型及其相关的实现，用于处理和生成大型数据集。用户指定一个 map 函数，该函数处理一个键/值对以生成一组中间键/值对，并指定一个 
reduce 函数，该函数合并所有与同一中间键相关的中间值。许多现实世界的任务都可以在这个模型中表示，如论文所示。在这个模型中，用户定义两个主要函数：map
和 reduce。具体来说： Map 函数：该函数接受一个键/值对作为输入，然后生成一组中间键/值对。其目的是对输入数据进行初步处理和转换。例如，如果输入数据
是网页日志，map 函数可以处理每一行日志记录，并生成特定的中间键/值对，如词频统计中的单词和出现次数。 Reduce 函数：该函数负责合并所有与同一中间键相
关联的中间值。其目的是对中间结果进行汇总和整理。例如，在词频统计的例子中，reduce 函数会将所有相同单词的出现次数加起来，得到每个单词在整个数据集中出
现的总次数。用这种函数式风格编写的程序会自动并行化，并在大型普通机器集群上执行。运行时系统负责处理输入数据的分区，调度程序在一组机器上的执行，处理机
器故障，以及管理所需的机器间通信。这使得没有并行和分布式系统经验的程序员也能轻松利用大型分布式系统的资源。我们的 MapReduce 实现运行在大型普通机器
集群上，并具有很高的可扩展性：一个典型的 MapReduce 计算处理数十 TB 的数据，运行在数千台机器上。程序员发现这个系统易于使用：已经实现了数百个 Map
Reduce 程序，每天在 Google 的集群上执行超过一千个 MapReduce 作业。Map 和 Reduce。 用户编写的 Map 函数接受一个输入对，并生成一组中间键/值对。
MapReduce 库将所有与相同中间键 I 相关联的中间值组合在一起，并将它们传递给 Reduce 函数。 同样由用户编写的 Reduce 函数接受一个中间键 I 和该键的
一组值。它将这些值合并形成可能较小的一组值。通常每次 Reduce 调用只产生零个或一个输出值。中间值通过一个迭代器提供给用户的 reduce 函数。这使我们能
够处理过大而无法装入内存的值列表。用户提供的 map 和 reduce 函数具有相关类型：即，输入键和值来自与输出键和值不同的域。此外，中间键和值与输出键和值
来自相同的域。C++ 实现将字符串传递给用户定义的函数并从中接收字符串，并由用户代码负责在字符串和适当类型之间进行转换。
中间键（Intermediate Key）
定义：中间键是Mapper处理输入数据后生成的键，用于对数据进行分组和排序。
作用：中间键用于标识Mapper输出的不同数据类别，使得具有相同中间键的数据能够被分配到同一个Reducer进行处理。
示例：在词频统计的例子中，如果输入是一个文本文件，Mapper会对文本进行分词，每个词就是一个中间键。例如：("word", 1)，其中"word"就是中间键。
中间值（Intermediate Value）
定义：中间值是Mapper处理输入数据后生成的值，与中间键配对。
作用：中间值是与中间键关联的数据内容，Reducer会根据中间键获取对应的中间值集合进行处理。
示例：继续词频统计的例子，Mapper会生成键值对("word", 1)，其中1就是中间值，表示该词出现一次
倒排索引（Inverted Index）是信息检索领域中常用的一种数据结构，用于快速查找包含特定词项（term）的文档。它将文档集合中每个词项出现的位置与之关联，
从而支持通过词项来查找文档。
**实现**
MapReduce 接口有许多不同的实现方式可供选择，选择适合的方式取决于环境。例如，一个实现可能适用于小型共享内存机器，另一个适用于大型 NUMA 多处理器系统，还有一个适用于更大规模的网络化机器集合。

本节描述了针对 Google 广泛使用的计算环境的一种实现：大规模的由廉价个人计算机（PC）通过交换式以太网连接在一起的集群 [4]。在我们的环境中：

机器通常是双处理器的 x86 处理器，运行 Linux，每台机器配备 2-4 GB 的内存。
使用廉价的网络硬件——通常是每台机器级别的 100 兆位/秒或 1 千兆位/秒，但总体双向带宽平均要低得多。
集群由数百或数千台机器组成，因此机器故障很常见。
存储由直接连接到个别机器的廉价 IDE 磁盘提供。我们使用一种内部开发的分布式文件系统 [8] 来管理存储在这些磁盘上的数据。文件系统使用复制来在不可靠的硬件上提供可用性和可靠性。
用户通过调度系统提交作业。每个作业包含一组任务，调度程序将其映射到集群中的一组可用机器。
**3.1 执行概述**
Map 调用通过自动将输入数据分区成 M 个片段来分布到多台机器上。不同机器可以并行处理输入片段。Reduce 调用通过使用分区函数（例如，hash(key) mod R）将中间键空间分区为 R 个片段来分布。分区数（R）和分区函数由用户指定。

图 1 显示了在我们的实现中 MapReduce 操作的整体流程。当用户程序调用 MapReduce 函数时，会发生以下一系列动作（图中编号与下面列表中的数字对应）：
用户程序中的 MapReduce 库首先将输入文件分割成 M 个片段，通常每个片段大小为 16 MB 到 64 MB（用户可通过可选参数进行控制）。然后在集群中的多台机器上启动许多程序副本。
程序的其中一个副本是特殊的——即主节点。其余的是工作节点，由主节点分配工作。有 M 个 map 任务和 R 个 reduce 任务要分配。主节点选择空闲的工作节点，并为每个节点分配一个 map 任务或 reduce 任务。
被分配了 map 任务的工作节点读取相应输入片段的内容。它从输入数据中解析出键/值对，并将每对传递给用户定义的 Map 函数。Map 函数产生的中间键/值对在内存中进行缓冲。
定期地，缓冲的键/值对会被写入本地磁盘，并按照分区函数分成 R 个区域。这些缓冲的键/值对的位置被传回主节点，主节点负责将这些位置转发给 reduce 工作节点。
当主节点通知 reduce 工作节点这些位置时，它使用远程过程调用（RPC）从 map 工作节点的本地磁盘上读取缓冲数据。当 reduce 工作节点读取完所有中间数据后，
它按中间键排序这些数据，以便所有相同键的出现都被分组在一起。由于通常许多不同的键映射到同一个 reduce 任务，因此需要排序。如果中间数据量过大，无法放入内存，则使用外部排序。
reduce 工作节点遍历排序后的中间数据，并对每个唯一的中间键遇到的对应的中间值集合传递给用户的 Reduce 函数。Reduce 函数的输出被追加到该 reduce 分区的最终输出文件中。
当所有 map 任务和 reduce 任务完成时，主节点唤醒用户程序。此时，用户程序中的 MapReduce 调用返回给用户代码。
成功完成后，mapreduce 执行的输出可用于 R 个输出文件中（每个 reduce 任务一个文件，文件名由用户指定）。通常，用户不需要将这些 R 个输出文件合并成
一个文件——他们通常将这些文件作为另一个 MapReduce 调用的输入，或者从另一个能处理被分成多个文件的输入的分布式应用中使用它们。
**3.2 主节点数据结构**
主节点维护多个数据结构。对于每个 map 任务和 reduce 任务，它存储状态（空闲、进行中或已完成），以及工作机器的身份（对于非空闲任务）。
主节点是中间文件区域位置从 map 任务传播到 reduce 任务的媒介。因此，对于每个完成的 map 任务，主节点存储由该 map 任务生成的 R 个中间文件区域的位置
和大小。随着 map 任务的完成，会收到对该位置和大小信息的更新。这些信息逐步推送给正在进行 reduce 任务的工作节点。
**3.3 容错性**
由于 MapReduce 库旨在使用数百或数千台机器处理大量数据，因此库必须能够优雅地容忍机器故障。
工作节点故障
主节点定期对每个工作节点进行 ping 测试。如果一段时间内未收到工作节点的响应，主节点将其标记为失败。由该工作节点完成的任何 map 任务将被重置为初始的
空闲状态，因此可以重新分配给其他工作节点。类似地，任何在故障工作节点上进行中的 map 任务或 reduce 任务也将被重置为空闲状态，并可以重新调度。

由于已完成的 map 任务的输出存储在故障机器的本地磁盘上，因此需要在故障时重新执行这些任务，因为输出是无法访问的。已完成的 reduce 任务不需要重新执行
，因为它们的输出存储在全局文件系统中。

当 map 任务首先由工作节点 A 执行，然后由于 A 失败而后由工作节点 B 执行时，所有执行 reduce 任务的工作节点都会被通知重新执行。任何尚未从工作节点 
A 读取数据的 reduce 任务将从工作节点 B 读取数据。MapReduce 对大规模工作节点故障具有韧性。例如，在一次 MapReduce 操作中，正在运行的集群的网络维护导致每次失效 80 台机器，持续几分钟。MapReduce 主节点简单地重新执行了不可达的工作节点机器的工作，并继续前进，最终完成了 MapReduce 操作。

主节点故障
主节点可以轻松地定期将上述主节点数据结构写入检查点。如果主节点任务终止，可以从最后一个检查点状态启动新副本。然而，由于只有一个主节点，其失败的可能性
很小；因此，我们的当前实现在主节点失败时会中止 MapReduce 计算。客户端可以检查此条件，并在需要时重试 MapReduce 操作。

故障存在时的语义
当用户提供的 map 和 reduce 操作符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的非故障顺序执行产生的输出相同。

我们依靠 map 和 reduce 任务输出的原子提交来实现这一属性。每个进行中的任务将其输出写入私有临时文件。一个 reduce 任务产生一个这样的文件，而一个 
map 任务产生 R 个这样的文件（每个 reduce 任务一个文件）。当一个 map 任务完成时，工作节点向主节点发送消息，并在消息中包括 R 个临时文件的名称。
如果主节点收到已完成 map 任务的完成消息，则忽略该消息。否则，它记录 R 个文件的名称在主节点数据结构中。

当一个 reduce 任务完成时，reduce 工作节点原子地将其临时输出文件重命名为最终输出文件。如果同一个 reduce 任务在多台机器上执行，那么同一个最终输出
文件将执行多次重命名调用。我们依赖底层文件系统提供的原子重命名操作来保证最终文件系统状态只包含一个 reduce 任务执行产生的数据。

我们的大多数 map 和 reduce 操作符都是确定性的，而且在这种情况下，我们的语义等同于非故障程序的顺序执行，使得程序员很容易理解他们程序行为。当 map 
和/或 reduce 操作符是非确定性的时，我们提供更弱但仍合理的语义。在非确定性操作符存在的情况下，特定 reduce 任务 R1 的输出等同于顺序执行非确定性程
序的 R1 产生的输出。然而，不同 reduce 任务 R2 的输出可能对应于顺序执行不同非确定性程序的 R2 产生的输出。

考虑 map 任务 M 和 reduce 任务 R1 和 R2。设 e(Ri) 表示提交的 Ri 执行（只有一种这样的执行）。弱语义的产生是因为 e(R1) 可能读取由 M 的一个执
行产生的输出，而 e(R2) 可能读取由 M 的不同执行产生的输出。
**3.4 本地性**
在我们的计算环境中，网络带宽是相对稀缺的资源。我们通过利用以下事实来节省网络带宽：输入数据（由 GFS [8] 管理）存储在构成我们集群的机器的本地磁盘上。
GFS 将每个文件分割成 64 MB 的块，并在不同的机器上存储多个副本（通常是 3 个副本）。MapReduce 主节点考虑到输入文件的位置信息，并尝试将 map 任务
调度到包含相应输入数据副本的机器上。如果失败，则尝试在与包含数据的机器相同网络交换机上附近的机器上调度 map 任务。在运行大规模 MapReduce 操作时，
集群中的大部分输入数据都可以本地读取，因此不会消耗网络带宽。
**3.5 任务粒度**
我们将 map 阶段划分为 M 个部分，将 reduce 阶段划分为 R 个部分，如上所述。理想情况下，M 和 R 应远远大于工作机器的数量。每个工作节点执行多个不同
的任务可以改善动态负载平衡，并加快在工作节点失败时的恢复速度：完成的许多 map 任务可以分布在所有其他工作节点上。

在我们的实现中，M 和 R 的大小有实际限制，因为主节点必须根据上述描述做出 O(M + R) 的调度决策，并在内存中保持 O(M * R) 的状态（内存使用的常数因子
很小：O(M * R) 状态部分大约为每个 map 任务/ reduce 任务对一字节的数据）。此外，R 通常受到用户限制，因为每个 reduce 任务的输出最终会进入单独的
输出文件。实际上，我们倾向于选择 M，使得每个单独的任务大约为 16 MB 到 64 MB 的输入数据（以便上述的本地性优化效果最好），并且我们将 R 设为预计要
使用的工作机器数的小倍数。我们通常使用 M = 200,000 和 R = 5,000，在 2,000 台工作机器上执行 MapReduce 计算。
**3.6 备份任务**
导致 MapReduce 操作总时间延长的常见原因之一是“拖沓者”：某台机器在计算中的最后几个 map 或 reduce 任务中花费的时间异常长。拖沓者可能由多种原因引
起。例如，一个具有坏磁盘的机器可能会频繁发生可纠正的错误，导致其读取性能从 30 MB/s 下降到 1 MB/s。集群调度系统可能已经在该机器上安排了其他任务，
导致由于 CPU、内存、本地磁盘或网络带宽的竞争而使其执行 MapReduce 代码变慢。我们最近遇到的问题是机器初始化代码中的一个 bug 导致处理器缓存被禁用：
受影响机器上的计算速度下降了一百倍以上。

我们有一般性机制来缓解拖沓者问题。当一个 MapReduce 操作接近完成时，主节点会安排剩余进行中任务的备份执行。只要主要执行或备份执行完成其中之一，任务
就被标记为已完成。我们已经调整了这个机制，使其通常将操作使用的计算资源增加不超过几个百分点。我们发现，这显著缩短了完成大规模 MapReduce 操作所需的
时间。例如，在第 5.3 节描述的排序程序中，禁用备份任务机制时，完成时间增加了 44%。
**4 优化**
虽然简单地编写 Map 和 Reduce 函数提供了大多数需求所需的基本功能，但我们发现一些扩展非常有用。本节将描述这些扩展。
**4.1 分区函数**
MapReduce 的用户指定他们期望的 reduce 任务/输出文件数量（R）。数据通过中间键的分区函数分配到这些任务中。提供了一个默认的分区函数，使用哈希函数
（例如“hash(key) mod R”）。这通常会导致分区相当均衡。然而，在某些情况下，通过键的某些其他函数进行数据分区是有用的。例如，有时输出键是 URL，我们
希望同一个主机的所有条目最终都进入同一个输出文件。为了支持这种情况，MapReduce 库的用户可以提供特殊的分区函数。例如，使用“hash(Hostname(urlkey)
mod R”作为分区函数将导致同一主机的所有 URL 最终进入同一个输出文件。
**4.2 排序保证**
我们保证在给定分区内，中间键/值对按键的递增顺序处理。这种排序保证使得能够为每个分区生成排序的输出文件变得容易。这在输出文件格式需要支持通过键进行高效
随机访问查找，或者用户希望数据排序方便时非常有用。
**4.3 合并器函数**
在某些情况下，由每个 map 任务生成的中间键存在显著重复，并且用户指定的 Reduce 函数是可交换和可结合的。Section 2.1 中的单词计数示例就是一个很好的
例子。由于单词频率往往遵循 Zipf 分布，每个 map 任务将生成形式为 <the, 1> 的数百到数千条记录。所有这些计数将通过网络发送到单个 reduce 任务，并
由 Reduce 函数相加以生成一个数字。我们允许用户指定一个可选的合并器函数，在数据发送到网络之前对其进行部分合并。
合并器函数在执行 map 任务的每台机器上执行。通常情况下，使用相同的代码来实现合并器和 reduce 函数。Reduce 函数的输出写入最终输出文件，而合并器函数
的输出写入中间文件，该文件将发送到 reduce 任务。
部分合并显著加快了某些类别的 MapReduce 操作。附录 A 包含一个使用合并器的示例。
**4.4 输入和输出类型**
MapReduce 库提供支持从多种不同格式读取输入数据的功能。例如，“text”模式输入将每行视为键/值对：键是文件中的偏移量，值是行的内容。另一种常见的支持格
式存储了按键排序的一系列键/值对。每种输入类型的实现都知道如何将自身拆分为用于处理的有意义的范围（例如，文本模式的范围拆分确保只在行边界处发生范围拆分）
。用户可以通过提供简单读取器接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少量预定义的输入类型之一。

读取器不一定需要提供从文件中读取的数据。例如，可以轻松定义一个从数据库或内存中映射的数据结构中读取记录的读取器。

类似地，我们支持一组输出类型，用于以不同格式生成数据，并且用户代码可以轻松添加对新输出类型的支持。
**4.5 副作用**
在某些情况下，MapReduce 的用户发现生成辅助文件作为其 map 和/或 reduce 运算符的附加输出非常方便。我们依赖于应用程序编写者使这些副作用具有原子性
和幂等性。通常，应用程序会将数据写入临时文件，并在完全生成后将此文件原子重命名。

我们不支持单个任务产生的多个输出文件的原子两阶段提交。因此，产生多个输出文件并具有跨文件一致性要求的任务应是确定性的。在实践中，这种限制从未成为问题。
**4.6 跳过坏记录**
有时，用户代码中存在导致 Map 或 Reduce 函数在某些记录上确定性崩溃的错误。这些错误会阻止 MapReduce 操作的完成。通常的做法是修复错误，但有时这是
不可行的；也许错误是在不可用源代码的第三方库中。此外，有时忽略几条记录是可以接受的，例如在大数据集上进行统计分析时。我们提供了一种可选的执行模式，
其中 MapReduce 库会检测导致确定性崩溃的记录，并跳过这些记录以便推进进程。

每个工作进程都安装了一个信号处理程序，用于捕获分段违规和总线错误。在调用用户 Map 或 Reduce 操作之前，MapReduce 库将参数的序列号存储在全局变量中。
如果用户代码生成信号，则信号处理程序会发送一个“最后的呼吸”UDP 数据包，其中包含序列号，发送到 MapReduce 主节点。当主节点在特定记录上看到多次失败时
，表明在下次重新执行相应的 Map 或 Reduce 任务时应跳过该记录。
**4.7 本地执行**
在分布式系统中进行 Map 或 Reduce 函数的调试可能会很棘手，因为实际计算通常在数千台机器上进行，主节点动态做出工作分配决策。为了帮助简化调试、分析和小
规模测试，我们开发了 MapReduce 库的另一种实现，该实现在本地机器上顺序执行所有 MapReduce 操作的工作。用户可以使用特殊标志调用其程序，然后可以轻松
使用任何他们发现有用的调试或测试工具（例如 gdb）。

**4.8 状态信息**
主节点运行一个内部 HTTP 服务器，并导出一组用于人工消费的状态页面。状态页面显示计算的进度，如已完成的任务数、进行中的任务数、输入字节、中间数据字节、
输出字节、处理速率等。页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用此数据预测计算需要多长时间，以及是否应向计算添加更多资源。
这些页面还可以用于确定计算是否比预期慢得多。

此外，顶级状态页面显示了哪些工作机器失败了，以及它们失败时正在处理的 map 和 reduce 任务。在试图诊断用户代码中的错误时，这些信息非常有用。
**4.8 状态信息**
主节点运行一个内部的 HTTP 服务器，并导出一组状态页面供人类消费。这些状态页面显示计算的进度，如已完成的任务数、正在进行的任务数、输入字节、中间数据字节、输出字节、处理速率等。页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算需要多长时间，以及是否需要向计算添加更多资源。这些页面还可以用于确定计算是否比预期慢得多。

此外，顶级状态页面显示哪些工作机器失败了，以及它们在失败时正在处理的 map 和 reduce 任务。这些信息在尝试诊断用户代码中的错误时非常有用。

**4.9 计数器**
MapReduce 库提供了一个计数器功能，用于计数各种事件的发生次数。例如，用户代码可能希望计算处理的总单词数或索引的德语文档数等。

要使用这个功能，用户代码创建一个命名计数器对象，然后在 Map 和/或 Reduce 函数中适当地增加计数器。例如：
`Counter* uppercase;
uppercase = GetCounter("uppercase");
map(String name, String contents):
for each word w in contents:
if (IsCapitalized(w)):
uppercase->Increment();
EmitIntermediate(w, "1");`
来自各个工作机器的计数器值定期传播到主节点（通过 ping 响应）。主节点汇总成功的 map 和 reduce 任务的计数器值，并在 MapReduce 
操作完成时将它们返回给用户代码。当前的计数器值也显示在主节点状态页面上，以便用户可以监视实时计算的进度。在汇总计数器值时，主节点消除相同 map 或 
reduce 任务的重复执行的影响，以避免重复计数（重复执行可能来自备份任务的使用或由于失败而重新执行任务）。

MapReduce 库自动维护一些计数器值，例如处理的输入键/值对数和生成的输出键/值对数。

用户发现计数器功能在检查 MapReduce 操作的行为时非常有用。例如，在某些 MapReduce 操作中，用户代码可能希望确保生成的输出对数恰好等于处理的输入对数，
或者处理的德语文档的比例在总文档数量中处于可接受的分数范围内。
**5 性能**
本节我们测量了在一个大型机器集群上运行的两个 MapReduce 计算的性能。一个计算是搜索大约一 TB 数据中的特定模式。另一个计算是对大约一 TB 数据进行排序。

**5.1 集群配置**
所有程序都在一个由大约 1800 台机器组成的集群上执行。每台机器配备两个 2GHz 的 Intel Xeon 处理器，启用了超线程技术，4GB 内存，两个 160GB 的 
IDE 硬盘，以及一个千兆以太网连接。这些机器以两级树形交换网络的形式布置，根节点处有大约 100-200 Gbps 的总带宽。所有机器都位于同一个托管设施中，因此任意两台机器之间的往返时间不到一毫秒。

每台机器的 4GB 内存中，大约有 1-1.5GB 被集群中运行的其他任务保留。程序在一个周末的下午执行，此时 CPU、磁盘和网络大多处于空闲状态。
**5.2 Grep**
grep 程序扫描大约 1010 个 100 字节的记录，搜索一个相对稀少的三字符模式（该模式出现在 92,337 个记录中）。输入被分成大约 64MB 的片段（M = 15000），整个输出被放置在一个文件中（R = 1）。

图 2 显示了计算随时间推移的进展情况。Y 轴显示了输入数据扫描的速率。随着更多的机器被分配给这个 MapReduce 计算，速率逐渐提高，当分配了 1764 个工作
机器时达到峰值，超过 30 GB/s。随着 map 任务完成，速率开始下降，并在计算进行约 80 秒时降为零。整个计算从开始到结束大约需要 150 秒。这包括大约一分
钟的启动开销。启动开销是由于程序传播到所有工作机器，并且与 GFS 交互以打开 1000 个输入文件并获取用于局部优化的信息。
**5.3 Sort**
sort 程序对大约 1010 个 100 字节的记录（约 1 TB 数据）进行排序。这个程序模仿了 TeraSort 基准测试 [10]。

排序程序只有不到 50 行用户代码。一个三行的 Map 函数从文本行中提取一个 10 字节的排序键，并将键和原始文本行作为中间键值对发射出去。我们使用内置的 
Identity 函数作为 Reduce 操作符。这个函数将中间键值对不变地作为输出键值对。最终排序的输出被写入一组 2 副本的 GFS 文件中（即，输出作为程序的输出写入 2 TB）。

与之前一样，输入数据被分成 64MB 的片段（M = 15000）。我们将排序输出分成 4000 个文件（R = 4000）。分区函数使用键的初始字节将其分离为 R 个部分。
对于这个基准测试，我们的分区函数具有关键分布的内置知识。在一般的排序程序中，我们将添加一个预处理的 MapReduce 操作，收集键的样本并使用样本键的分布计算最终排序传递的分割点。

图 3(a) 显示了排序程序正常执行的进展情况。左上角的图表显示了读取输入的速率。速率峰值约为 13 GB/s，并在大约 200 秒之前快速下降，因为所有的 map 
任务在大约 200 秒内完成。请注意，输入速率低于 grep 的速率。这是因为排序的 map 任务大约一半时间和 I/O 带宽用于将中间输出写入到它们的本地磁盘。相应的 grep 的中间输出大小可忽略不计。

中间左侧的图表显示了从 map 任务发送数据到 reduce 任务的网络传输速率。这种洗牌从第一个 map 任务完成时开始。图表中的第一个峰值是大约 1700 个 
reduce 任务的第一批（整个 MapReduce 大约分配了 1700 台机器，并且每台机器最多同时执行一个 reduce 任务）。大约 300 秒后，一些第一批 reduce 
任务完成，我们开始为剩余的 reduce 任务洗牌数据。所有的洗牌在计算进行约 600 秒时完成。

左下角的图表显示了 reduce 任务将排序数据写入最终输出文件的速率。在第一个洗牌期结束和写入期开始之间存在延迟，因为机器忙于对中间数据进行排序。
写入速率继续保持在大约 2-4 GB/s 一段时间。所有的写入在计算进行约 850 秒时完成。包括启动开销，整个计算需要 891 秒。这与当前最佳的 TeraSort 基准测试结果（1057 秒）相似。

需要注意的几点：输入速率高于洗牌速率和输出速率，这是由于我们的局部优化 - 大多数数据从本地磁盘读取，绕过了相对带宽受限的网络。洗牌速率高于输出速率，
因为输出阶段写入排序数据的两个副本（出于可靠性和可用性原因，我们制作两个输出的副本）。我们写入两个副本，因为这是底层文件系统提供的可靠性和可用性机制。
如果底层文件系统使用纠错编码 [14] 而不是复制，写入数据的网络带宽要求将会降低。
**5.4 备份任务的影响**
在图 3 (b) 中，我们展示了禁用备份任务的排序程序执行过程。执行流程与图 3 (a) 中显示的类似，但存在一个非常长的尾部，几乎没有写活动发生。在经过 960 
秒后，除了 5 个 reduce 任务外，所有任务都已完成。然而，这最后几个拖延者直到 300 秒后才完成。整个计算花费了 1283 秒，增加了 44% 的总计算时间。

**5.5 机器故障**
在图 3 (c) 中，我们展示了排序程序的执行过程，其中我们故意在计算进行几分钟后杀死了 200 台 1746 台工作进程。底层的集群调度器立即在这些机器上重新启动
了新的工作进程（因为只有进程被杀死，机器仍然正常运行）。

工作进程的死亡表现为负的输入速率，因为一些先前完成的 map 工作消失了（因为对应的 map 工作者被杀死），需要重新执行。这些 map 工作的重新执行相对较快。
整个计算包括启动开销在内在 933 秒内完成（仅比正常执行时间增加了 5%）。
**急切调度机制**（Eager Scheduling）是一种用于并行和分布式计算的任务调度策略，其目的是通过尽早启动备份任务来应对任务的潜在故障或延迟，从而加快整个计算过程的完成时间。这种机制尤其在处理长尾任务（那些花费时间远超其他任务的任务）和应对任务失败时非常有效。

急切调度机制的原理：
并行任务执行：

系统将多个任务分配到不同的工作节点并行执行。
每个任务开始时都按照正常的调度策略被分配给可用的节点。
备份任务：

为了应对可能的任务失败或延迟，系统会在检测到某些任务进展缓慢或可能失败时，启动这些任务的备份副本。
这些备份任务被分配到其他空闲的或低负载的节点上，以尽可能早地完成这些任务。
结果采集：

一旦某个任务（原始任务或其备份任务）完成，系统会接受其结果并终止其他正在进行的备份任务，以避免重复计算和资源浪费。
急切调度机制的优点：
提高容错性：通过启动备份任务，系统能够在原始任务失败或进展缓慢时，依然确保计算能够顺利完成。
减少长尾效应：在并行计算中，少数几个长时间执行的任务会影响整个计算过程的完成时间。急切调度机制通过启动备份任务来减少这些长尾任务的影响。
急切调度机制的缺点：
资源浪费：启动备份任务需要额外的计算资源，这可能会导致系统资源的浪费，特别是在备份任务不需要时。
复杂度增加：管理和调度备份任务增加了系统的复杂性，需要有效的监控和调度策略。
**7 相关工作**
许多系统提供了受限的编程模型，并利用这些限制自动并行化计算。例如，可以使用并行前缀计算在 N 个处理器上以 log N 时间计算 N 元素数组的所有前缀的关联
函数 [6, 9, 13]。MapReduce 可以被视为基于我们在大规模实际计算中的经验，对这些模型的简化和提炼。更重要的是，我们提供了一个容错实现，可以扩展到数千个处理器。

与此相反，大多数并行处理系统仅在较小规模上实施，并将处理机器故障的细节留给程序员。Bulk Synchronous Programming [17] 和一些 MPI 原语 [11] 
提供了更高级别的抽象，使程序员编写并行程序更加容易。MapReduce 与这些系统的一个关键区别在于，MapReduce 利用受限的编程模型自动并行化用户程序，并提供透明的容错机制。

我们的本地化优化从诸如主动磁盘 [12, 15] 的技术中汲取灵感，其中计算被推送到接近本地磁盘的处理元素，以减少通过 I/O 子系统或网络发送的数据量。我们在
通用处理器上运行，其中少量磁盘直接连接，而不是直接在磁盘控制器处理器上运行，但总体方法类似。

我们的备份任务机制类似于 Charlotte 系统 [3] 中采用的急切调度机制。简单急切调度的一个缺点是，如果某个任务导致重复故障，则整个计算将无法完成。我们
通过跳过坏记录的机制修复了这个问题的一些实例。

MapReduce 实现依赖于一种内部集群管理系统，负责在大量共享机器上分发和运行用户任务。尽管不是本文的重点，但集群管理系统在精神上类似于其他系统，如 Condor [16]。

MapReduce 库中的排序设施操作类似于 NOW-Sort [1]。源机器（map workers）将要排序的数据分区，并将其发送给 R 个 reduce workers 中的一个。每个
reduce worker 在本地排序其数据（如果可能的话在内存中）。当然，NOW-Sort 不具备用户可定义的 Map 和 Reduce 函数，这使得我们的库具有广泛的适用性。

River [2] 提供了一种编程模型，其中进程通过发送数据到分布式队列来进行通信。与 MapReduce 类似，River 系统试图在异构硬件或系统扰动引入的非均匀性
的情况下提供良好的平均性能。River 通过精心安排磁盘和网络传输来实现平衡的完成时间。MapReduce 的方法有所不同。通过限制编程模型，MapReduce 框架能
够将问题分解为大量细粒度的任务。这些任务会动态地调度到可用的工作机器上，使更快的工作机器处理更多任务。受限制的编程模型还允许我们在作业结束时安排任务
的冗余执行，大大减少了在存在非均匀性（例如慢速或卡住的工作机器）时的完成时间。

BAD-FS [5] 与 MapReduce 有非常不同的编程模型，并且不像 MapReduce，它针对在广域网上执行作业。然而，两者有两个基本相似之处。第一，这些系统都使用
冗余执行来从由故障引起的数据丢失中恢复。第二，它们都使用具有本地感知调度的方法来减少通过拥塞的网络链路发送的数据量。

TACC [7] 是一个旨在简化高可用网络服务构建的系统。与 MapReduce 类似，它依赖于重新执行作为实现容错的机制。

**8 结论**
MapReduce 编程模型在 Google 内部成功地用于多种不同的目的。我们将此成功归因于几个原因。首先，该模型易于使用，即使对于没有并行和分布式系统经验的
程序员，因为它隐藏了并行化、容错、本地化优化和负载平衡的细节。其次，大量问题都可以轻松表达为 MapReduce 计算。例如，MapReduce 用于生成 Google 
生产网页搜索服务的数据、排序、数据挖掘、机器学习等多个系统。第三，我们开发了一个能够扩展到数千台机器组成的大型集群的 MapReduce 实现。该实现有效地
利用这些机器资源，因此适合用于 Google 遇到的许多大规模计算问题。

我们从这项工作中学到了几件事。首先，限制编程模型使得并行化和分发计算以及使这些计算容错变得容易。其次，网络带宽是一种稀缺资源。因此，我们系统中的许多
优化都旨在减少通过网络发送的数据量：本地化优化允许我们从本地磁盘读取数据，将中间数据写入本地磁盘的单个副本节省了网络带宽。第三，冗余执行可以用来减少
慢速机器的影响，并处理机器故障和数据丢失。
